2023-11-22 21:16:32,711 INFO: 
     ______                   __   __                 __      __
    / ____/____   ____   ____/ /  / /   __  __ _____ / /__   / /
   / / __ / __ \ / __ \ / __  /  / /   / / / // ___// //_/  / /
  / /_/ // /_/ // /_/ // /_/ /  / /___/ /_/ // /__ / /<    /_/
  \____/ \____/ \____/ \____/  /_____/\____/ \___//_/|_|  (_)
    
Version Information: 
	PyTorch: 2.0.1
	TorchVision: 0.15.2
2023-11-22 21:16:32,712 INFO: 
  name: ClassicalX2_014_Pretrained_SwinIR_MS-FFN_DIV2K_P48W8_LR2e-4_B8G1_titan_cos
  model_type: SwinIRModel
  scale: 2
  num_gpu: 1
  manual_seed: 3407
  datasets:[
    train:[
      name: DIV2K
      type: PairedImageDataset
      dataroot_gt: datasets/DIV2K/DIV2K_train_HR_sub
      dataroot_lq: datasets/DIV2K/DIV2K_train_LR_bicubic/X2_sub
      meta_info_file: data/meta_info/meta_info_DIV2K800sub_GT.txt
      filename_tmpl: {}
      io_backend:[
        type: disk
      ]
      gt_size: 96
      use_hflip: True
      use_rot: True
      num_worker_per_gpu: 8
      batch_size_per_gpu: 5
      dataset_enlarge_ratio: 1
      prefetch_mode: None
      phase: train
      scale: 2
    ]
    val_1:[
      name: Set5
      type: PairedImageDataset
      dataroot_gt: datasets/Test/HR/Set5/x2
      dataroot_lq: datasets/Test/LR/Set5/x2
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 2
    ]
    val_2:[
      name: Set14
      type: PairedImageDataset
      dataroot_gt: datasets/Test/HR/Set14/x2
      dataroot_lq: datasets/Test/LR/Set14/x2
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 2
    ]
    val_3:[
      name: BSD100
      type: PairedImageDataset
      dataroot_gt: datasets/Test/HR/BSD100/x2
      dataroot_lq: datasets/Test/LR/BSD100/x2
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 2
    ]
    val_4:[
      name: Urban100
      type: PairedImageDataset
      dataroot_gt: datasets/Test/HR/Urban100/x2
      dataroot_lq: datasets/Test/LR/Urban100/x2
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 2
    ]
    val_5:[
      name: Manga109
      type: PairedImageDataset
      dataroot_gt: datasets/Test/HR/Manga109/x2
      dataroot_lq: datasets/Test/LR/Manga109/x2
      io_backend:[
        type: disk
      ]
      phase: val
      scale: 2
    ]
  ]
  network_g:[
    type: SwinIR
    upscale: 2
    in_chans: 3
    img_size: 48
    window_size: 16
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
  ]
  path:[
    pretrain_network_g: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/weights/net_g_80000.pth
    strict_load_g: False
    resume_state: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/training_states/80000.state
    experiments_root: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti
    models: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/weights
    training_states: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/training_states
    log: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti
    visualization: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/visualization
    tb_logger: /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/tb_logger
  ]
  train:[
    ema_decay: 0.999
    optim_g:[
      type: Adam
      lr: 0.0002
      weight_decay: 0
      betas: [0.9, 0.99]
    ]
    scheduler:[
      type: CosineAnnealingRestartLR
      periods: [200000, 200000, 200000, 200000, 200000]
      restart_weights: [1, 0.5, 0.5, 0.5, 0.5]
    ]
    total_iter: 1000000
    warmup_iter: -1
    pixel_opt:[
      type: L1Loss
      loss_weight: 1.0
      reduction: mean
    ]
  ]
  val:[
    val_freq: 10000.0
    save_img: False
    metrics:[
      psnr:[
        type: calculate_psnr
        crop_border: 2
        test_y_channel: True
      ]
      ssim:[
        type: calculate_ssim
        crop_border: 2
        test_y_channel: True
      ]
    ]
  ]
  logger:[
    print_freq: 100
    save_checkpoint_freq: 10000.0
    use_tb_logger: True
    wandb:[
      project: None
      resume_id: None
    ]
  ]
  dist_params:[
    backend: nccl
    port: 29500
  ]
  dist: False
  rank: 0
  world_size: 1
  auto_resume: True
  is_train: True
  root_path: /share3/home/renzihao

2023-11-22 21:16:50,271 INFO: Dataset [PairedImageDataset] - DIV2K is built.
2023-11-22 21:16:50,271 INFO: Training statistics:
	Number of train images: 32592
	Dataset enlarge ratio: 1
	Batch size per gpu: 5
	World size (gpu number): 1
	Require iter number per epoch: 6519
	Total epochs: 154; iters: 1000000.
2023-11-22 21:16:50,272 INFO: Dataset [PairedImageDataset] - Set5 is built.
2023-11-22 21:16:50,272 INFO: Number of val images/folders in Set5: 5
2023-11-22 21:16:50,273 INFO: Dataset [PairedImageDataset] - Set14 is built.
2023-11-22 21:16:50,273 INFO: Number of val images/folders in Set14: 14
2023-11-22 21:16:50,278 INFO: Dataset [PairedImageDataset] - BSD100 is built.
2023-11-22 21:16:50,278 INFO: Number of val images/folders in BSD100: 100
2023-11-22 21:16:50,282 INFO: Dataset [PairedImageDataset] - Urban100 is built.
2023-11-22 21:16:50,282 INFO: Number of val images/folders in Urban100: 100
2023-11-22 21:16:50,288 INFO: Dataset [PairedImageDataset] - Manga109 is built.
2023-11-22 21:16:50,288 INFO: Number of val images/folders in Manga109: 109
2023-11-22 21:16:51,889 INFO: Network: SwinIR, with parameters: 13,051,943
2023-11-22 21:16:51,889 INFO: SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1-5): 5 x RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=16, shift_size=8, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(16, 16), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate='none')
              (dwconv): dwconv(
                (conv1): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(1, 1), stride=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv3): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv5): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=360)
                  (1): GELU(approximate='none')
                )
                (conv7): Sequential(
                  (0): Conv2d(360, 360, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=360)
                  (1): GELU(approximate='none')
                )
              )
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)
2023-11-22 21:17:01,846 INFO: Loading SwinIR model from /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/weights/net_g_80000.pth, with param key: [params].
2023-11-22 21:17:01,993 INFO: Use Exponential Moving Average with decay: 0.999
2023-11-22 21:17:02,445 INFO: Loading SwinIR model from /share3/home/renzihao/Pipeline/experiments/Doing/015_Pretrained_SwinIR_MS-FFN_DIV2K_P48W16_LR2e-4_cos_2080ti/weights/net_g_80000.pth, with param key: [params_ema].
2023-11-22 21:17:02,584 INFO: Loss [L1Loss] is created.
2023-11-22 21:17:02,587 INFO: Model [SwinIRModel] is created.
2023-11-22 21:17:02,811 INFO: Resuming training from epoch: 26, iter: 80000.
2023-11-22 21:17:03,323 INFO: Start training from epoch: 26, iter: 80000
2023-11-22 21:18:03,386 INFO: [Class..][epoch: 26, iter:  80,100, lr:(1.308e-04,)] [eta: 6 days, 9:14:59, time (data): 0.601 (0.023)] l_pix: 1.5270e-02 
2023-11-22 21:19:02,202 INFO: [Class..][epoch: 26, iter:  80,200, lr:(1.306e-04,)] [eta: 6 days, 7:45:43, time (data): 0.594 (0.013)] l_pix: 9.0635e-03 
2023-11-22 21:20:01,597 INFO: [Class..][epoch: 26, iter:  80,300, lr:(1.305e-04,)] [eta: 6 days, 7:44:34, time (data): 0.594 (0.002)] l_pix: 1.8449e-02 
2023-11-22 21:21:01,202 INFO: [Class..][epoch: 26, iter:  80,400, lr:(1.303e-04,)] [eta: 6 days, 7:51:31, time (data): 0.595 (0.002)] l_pix: 1.1610e-02 
2023-11-22 21:22:00,871 INFO: [Class..][epoch: 26, iter:  80,500, lr:(1.302e-04,)] [eta: 6 days, 7:57:16, time (data): 0.597 (0.002)] l_pix: 1.3917e-02 
2023-11-22 21:23:00,346 INFO: [Class..][epoch: 26, iter:  80,600, lr:(1.300e-04,)] [eta: 6 days, 7:55:49, time (data): 0.596 (0.002)] l_pix: 1.3741e-02 
2023-11-22 21:23:59,733 INFO: [Class..][epoch: 26, iter:  80,700, lr:(1.299e-04,)] [eta: 6 days, 7:52:35, time (data): 0.594 (0.002)] l_pix: 2.1592e-02 
2023-11-22 21:24:59,056 INFO: [Class..][epoch: 26, iter:  80,800, lr:(1.297e-04,)] [eta: 6 days, 7:48:40, time (data): 0.593 (0.002)] l_pix: 6.1956e-03 
2023-11-22 21:25:58,756 INFO: [Class..][epoch: 26, iter:  80,900, lr:(1.296e-04,)] [eta: 6 days, 7:51:49, time (data): 0.597 (0.002)] l_pix: 1.4746e-02 
2023-11-22 21:26:58,977 INFO: [Class..][epoch: 26, iter:  81,000, lr:(1.294e-04,)] [eta: 6 days, 8:02:07, time (data): 0.600 (0.002)] l_pix: 1.2341e-02 
2023-11-22 21:27:59,795 INFO: [Class..][epoch: 26, iter:  81,100, lr:(1.293e-04,)] [eta: 6 days, 8:18:41, time (data): 0.608 (0.002)] l_pix: 8.1752e-03 
2023-11-22 21:29:00,915 INFO: [Class..][epoch: 26, iter:  81,200, lr:(1.291e-04,)] [eta: 6 days, 8:36:09, time (data): 0.610 (0.002)] l_pix: 1.2735e-02 
2023-11-22 21:30:01,920 INFO: [Class..][epoch: 26, iter:  81,300, lr:(1.290e-04,)] [eta: 6 days, 8:49:26, time (data): 0.610 (0.002)] l_pix: 1.0739e-02 
2023-11-22 21:31:02,287 INFO: [Class..][epoch: 26, iter:  81,400, lr:(1.288e-04,)] [eta: 6 days, 8:53:42, time (data): 0.607 (0.002)] l_pix: 1.3101e-02 
2023-11-22 21:32:02,270 INFO: [Class..][epoch: 26, iter:  81,500, lr:(1.287e-04,)] [eta: 6 days, 8:53:20, time (data): 0.600 (0.002)] l_pix: 7.4953e-03 
2023-11-22 21:33:02,071 INFO: [Class..][epoch: 26, iter:  81,600, lr:(1.285e-04,)] [eta: 6 days, 8:51:10, time (data): 0.599 (0.002)] l_pix: 1.0578e-02 
2023-11-22 21:34:01,345 INFO: [Class..][epoch: 26, iter:  81,700, lr:(1.284e-04,)] [eta: 6 days, 8:44:23, time (data): 0.593 (0.002)] l_pix: 2.3792e-02 
2023-11-22 21:35:00,258 INFO: [Class..][epoch: 26, iter:  81,800, lr:(1.282e-04,)] [eta: 6 days, 8:35:11, time (data): 0.591 (0.002)] l_pix: 7.1737e-03 
2023-11-22 21:35:59,276 INFO: [Class..][epoch: 26, iter:  81,900, lr:(1.281e-04,)] [eta: 6 days, 8:27:42, time (data): 0.590 (0.002)] l_pix: 9.1564e-03 
2023-11-22 21:36:57,770 INFO: [Class..][epoch: 26, iter:  82,000, lr:(1.279e-04,)] [eta: 6 days, 8:16:51, time (data): 0.587 (0.002)] l_pix: 1.2525e-02 
2023-11-22 21:37:55,299 INFO: [Class..][epoch: 26, iter:  82,100, lr:(1.277e-04,)] [eta: 6 days, 7:59:55, time (data): 0.575 (0.002)] l_pix: 1.9079e-02 
2023-11-22 21:38:52,435 INFO: [Class..][epoch: 26, iter:  82,200, lr:(1.276e-04,)] [eta: 6 days, 7:41:42, time (data): 0.573 (0.002)] l_pix: 1.3127e-02 
2023-11-22 21:39:49,372 INFO: [Class..][epoch: 26, iter:  82,300, lr:(1.274e-04,)] [eta: 6 days, 7:23:39, time (data): 0.569 (0.002)] l_pix: 1.2498e-02 
2023-11-22 21:40:46,354 INFO: [Class..][epoch: 26, iter:  82,400, lr:(1.273e-04,)] [eta: 6 days, 7:07:20, time (data): 0.570 (0.002)] l_pix: 4.7586e-03 
2023-11-22 21:41:42,797 INFO: [Class..][epoch: 26, iter:  82,500, lr:(1.271e-04,)] [eta: 6 days, 6:48:56, time (data): 0.564 (0.002)] l_pix: 8.8707e-03 
2023-11-22 21:42:39,203 INFO: [Class..][epoch: 26, iter:  82,600, lr:(1.270e-04,)] [eta: 6 days, 6:31:40, time (data): 0.564 (0.002)] l_pix: 5.8770e-03 
2023-11-22 21:43:35,839 INFO: [Class..][epoch: 26, iter:  82,700, lr:(1.268e-04,)] [eta: 6 days, 6:16:55, time (data): 0.567 (0.002)] l_pix: 1.1585e-02 
2023-11-22 21:44:32,408 INFO: [Class..][epoch: 26, iter:  82,800, lr:(1.267e-04,)] [eta: 6 days, 6:02:47, time (data): 0.566 (0.002)] l_pix: 6.1882e-03 
2023-11-22 21:45:29,198 INFO: [Class..][epoch: 26, iter:  82,900, lr:(1.265e-04,)] [eta: 6 days, 5:50:43, time (data): 0.568 (0.002)] l_pix: 1.0301e-02 
2023-11-22 21:46:26,195 INFO: [Class..][epoch: 26, iter:  83,000, lr:(1.264e-04,)] [eta: 6 days, 5:40:27, time (data): 0.569 (0.002)] l_pix: 1.9208e-02 
2023-11-22 21:47:23,669 INFO: [Class..][epoch: 26, iter:  83,100, lr:(1.262e-04,)] [eta: 6 days, 5:33:08, time (data): 0.575 (0.002)] l_pix: 1.2926e-02 
2023-11-22 21:48:21,842 INFO: [Class..][epoch: 26, iter:  83,200, lr:(1.261e-04,)] [eta: 6 days, 5:29:33, time (data): 0.579 (0.002)] l_pix: 1.2305e-02 
2023-11-22 21:49:20,452 INFO: [Class..][epoch: 26, iter:  83,300, lr:(1.259e-04,)] [eta: 6 days, 5:28:09, time (data): 0.586 (0.002)] l_pix: 1.1967e-02 
2023-11-22 21:50:18,983 INFO: [Class..][epoch: 26, iter:  83,400, lr:(1.258e-04,)] [eta: 6 days, 5:26:25, time (data): 0.586 (0.002)] l_pix: 5.1116e-03 
2023-11-22 21:51:17,579 INFO: [Class..][epoch: 26, iter:  83,500, lr:(1.256e-04,)] [eta: 6 days, 5:25:01, time (data): 0.586 (0.002)] l_pix: 1.0959e-02 
2023-11-22 21:52:15,954 INFO: [Class..][epoch: 26, iter:  83,600, lr:(1.255e-04,)] [eta: 6 days, 5:22:42, time (data): 0.585 (0.002)] l_pix: 8.6911e-03 
2023-11-22 21:53:14,028 INFO: [Class..][epoch: 26, iter:  83,700, lr:(1.253e-04,)] [eta: 6 days, 5:19:12, time (data): 0.581 (0.002)] l_pix: 1.4928e-02 
2023-11-22 21:54:12,569 INFO: [Class..][epoch: 26, iter:  83,800, lr:(1.252e-04,)] [eta: 6 days, 5:17:44, time (data): 0.583 (0.002)] l_pix: 1.5374e-02 
2023-11-22 21:55:10,585 INFO: [Class..][epoch: 26, iter:  83,900, lr:(1.250e-04,)] [eta: 6 days, 5:14:13, time (data): 0.580 (0.002)] l_pix: 1.3076e-02 
2023-11-22 21:56:08,352 INFO: [Class..][epoch: 26, iter:  84,000, lr:(1.249e-04,)] [eta: 6 days, 5:09:54, time (data): 0.579 (0.002)] l_pix: 9.2333e-03 
2023-11-22 21:57:05,866 INFO: [Class..][epoch: 26, iter:  84,100, lr:(1.247e-04,)] [eta: 6 days, 5:04:47, time (data): 0.575 (0.002)] l_pix: 7.8145e-03 
2023-11-22 21:58:03,367 INFO: [Class..][epoch: 26, iter:  84,200, lr:(1.246e-04,)] [eta: 6 days, 4:59:49, time (data): 0.575 (0.002)] l_pix: 1.6088e-02 
2023-11-22 21:59:00,576 INFO: [Class..][epoch: 26, iter:  84,300, lr:(1.244e-04,)] [eta: 6 days, 4:54:01, time (data): 0.572 (0.002)] l_pix: 1.3410e-02 
2023-11-22 21:59:57,706 INFO: [Class..][epoch: 26, iter:  84,400, lr:(1.243e-04,)] [eta: 6 days, 4:48:09, time (data): 0.572 (0.002)] l_pix: 1.5197e-02 
2023-11-22 22:00:54,786 INFO: [Class..][epoch: 26, iter:  84,500, lr:(1.241e-04,)] [eta: 6 days, 4:42:21, time (data): 0.571 (0.002)] l_pix: 1.6813e-02 
2023-11-22 22:01:52,004 INFO: [Class..][epoch: 26, iter:  84,600, lr:(1.240e-04,)] [eta: 6 days, 4:37:12, time (data): 0.572 (0.002)] l_pix: 9.5800e-03 
2023-11-22 22:02:49,059 INFO: [Class..][epoch: 26, iter:  84,700, lr:(1.238e-04,)] [eta: 6 days, 4:31:42, time (data): 0.571 (0.002)] l_pix: 6.5150e-03 
